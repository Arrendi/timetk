% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune-plot_tune_parameter_ranking.R
\name{plot_tune_rank_parameters}
\alias{plot_tune_rank_parameters}
\title{Visualize a Hyperparameter Rankings}
\usage{
plot_tune_rank_parameters(
  .data,
  .point_alpha = 0.6,
  .point_color_best = "#2C3E50",
  .point_color_worst = "#E31A1C",
  .point_size_best = 4,
  .point_size_worst = 2,
  .title = "Hyperparameter Model Ranking & Selection",
  .color_lab = "Failure Rate Rank",
  .size_lab = "Variability Rank",
  .interactive = TRUE
)
}
\arguments{
\item{.data}{A \code{tibble} of class "tune_results"}

\item{.point_alpha}{Opacity for points. Set to 0.6 by default.}

\item{.point_color_best}{Color of point for best model rank}

\item{.point_color_worst}{Color of point for worst model rank}

\item{.point_size_best}{Size of point for the best model rank}

\item{.point_size_worst}{Size of point for the worst model rank}

\item{.title}{Plot title}

\item{.color_lab}{Legend label for color (Failure Rate Ranking)}

\item{.size_lab}{Legend label for the size (Standard Error Ranking)}

\item{.interactive}{Toggle between interactive plotly chart and static ggplot chart}
}
\description{
The \code{plot_tune_rank_parameters()} function provides a visualization
for time series hyperparameter tuning results (\code{tune_results}) of either \code{rolling_origin}
or \code{time_series_cv} class that have been tuned.
}
\details{
\strong{Metric Ranking (Model Accuracy, Y-Axis)}

The model with the lowest (best) rank is that with the lowest mean error. Y-Axis is the Metric value.
\itemize{
\item \strong{Metric Calculation} - Refer to the appropriate metric from the \code{yardstick} R package.
\item \strong{Metric Ranking} - Sort Best to Worst mean metric. Rank 1 to N.
}

\strong{Failure Rate Ranking (Robustness to New Data, Color)}

Models with lower failure rates are more robust to new data. Color is the Failure Rate.
Failure rate rank is a score based on the proportion of models that failed during tuning.
\itemize{
\item \strong{Failure Rate Calculation:} \emph{Failure Rate = n / No. of Resample Slices}
\item \strong{Failure Rate Ranking:} Use Min Ranking (\code{dplyr::min_rank()}) on the Failure Rate Calculation.
}

\strong{Standard Error Ranking (Model Variability, Size)}

Models with lower standard error are more consistent (less variability). Size of the point is
the standard error ranking.
\itemize{
\item \strong{Standard Error Calculation} - See \code{base::sd()}.
\item \strong{Standard Error Ranking} - Sort Best to Worst standard error. Rank 1 to N.
}
}
\examples{
library(dplyr)
library(tune)
library(timetk)

arima_workflow_tuned \%>\%
    tk_tune_rank_parameters() \%>\%
    plot_tune_rank_parameters(.interactive = FALSE)

}
\seealso{
\itemize{
\item \link{tk_tune_rank_parameters} - Ranking parameters
\item \link{tk_tune_select_parameters} - Selecting the best parameters
}
}
